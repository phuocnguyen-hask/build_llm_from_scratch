{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0325462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b4c71",
   "metadata": {},
   "source": [
    "**Attention mechanism**\n",
    "1. Produce queries ,keys, values,\n",
    "2. Calculate `attention_scores`, mask those above diagonal to prevent cheating\n",
    "3. Put `attention_scores` into softmax function, then do weighted sum of values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cded4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, cfg, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        d_in = cfg['n_embd']\n",
    "        d_out = cfg['n_embd']\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Step1: produce queries, keys, values\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Step2: calculate attention_scores\n",
    "        attn_scores = queries @ keys.transpose(-1, -2)\n",
    "        mask = torch.tril(torch.ones_like(attn_scores), diagonal=0)\n",
    "        attn_scores.masked_fill_(mask==0, -torch.inf)\n",
    "\n",
    "        # Step3\n",
    "        weight_scores = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_out = weight_scores @ values\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "013ddb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_CONFIG = {\n",
    "  \"activation_function\": \"gelu_new\",\n",
    "  \"architectures\": [\n",
    "    \"GPT2LMHeadModel\"\n",
    "  ],\n",
    "  \"attn_pdrop\": 0.1,\n",
    "  \"bos_token_id\": 50256,\n",
    "  \"embd_pdrop\": 0.1,\n",
    "  \"eos_token_id\": 50256,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_epsilon\": 1e-05,\n",
    "  \"model_type\": \"gpt2\",\n",
    "  \"n_ctx\": 1024,\n",
    "  \"n_embd\": 768,\n",
    "  \"n_head\": 12,\n",
    "  \"n_layer\": 12,\n",
    "  \"n_positions\": 1024,\n",
    "  \"resid_pdrop\": 0.1,\n",
    "  \"summary_activation\": None,\n",
    "  \"summary_first_dropout\": 0.1,\n",
    "  \"summary_proj_to_labels\": True,\n",
    "  \"summary_type\": \"cls_index\",\n",
    "  \"summary_use_proj\": True,\n",
    "  \"task_specific_params\": {\n",
    "    \"text-generation\": {\n",
    "      \"do_sample\": True,\n",
    "      \"max_length\": 50\n",
    "    }\n",
    "  },\n",
    "  \"vocab_size\": 50257\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad024c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "print(tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34cab7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 768])\n",
      "tensor([[ 3.1114,  0.4252, -1.0459,  ..., -1.6001,  0.7646,  0.4166],\n",
      "        [ 0.7925,  1.1849, -0.0425,  ...,  1.8415, -0.8759, -3.4306],\n",
      "        [-0.5999,  1.8658, -0.6810,  ..., -0.0278, -0.5934, -2.6497],\n",
      "        ...,\n",
      "        [ 0.6929, -1.1271, -1.5086,  ...,  0.0370,  1.1239, -1.1810],\n",
      "        [ 0.0143,  2.1574, -0.2989,  ..., -0.2977,  1.8317,  0.3929],\n",
      "        [-0.2165, -1.9488,  0.0999,  ...,  0.1306, -3.2307, -1.2602]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(GPT2_CONFIG['vocab_size'], GPT2_CONFIG['n_embd'])\n",
    "pos_embedding_layer = nn.Embedding(GPT2_CONFIG['n_positions'], GPT2_CONFIG['n_embd'])\n",
    "txt = 'Do you know who am i?'\n",
    "tokens =  tokenizer.encode(txt)\n",
    "token_len = len(tokens)\n",
    "tokens = torch.tensor(tokens) # convert to pytorch tensor for compapility\n",
    "embeded_vec = embedding_layer(tokens)\n",
    "pos_vec = embedding_layer(torch.arange(token_len))\n",
    "input_vec = embeded_vec + pos_vec\n",
    "print(input_vec.shape)\n",
    "print(input_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8b3f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 768])\n",
      "tensor([[ 3.1114,  0.4252, -1.0459,  ..., -1.6001,  0.7646,  0.4166],\n",
      "        [ 0.7925,  1.1849, -0.0425,  ...,  1.8415, -0.8759, -3.4306],\n",
      "        [-0.5999,  1.8658, -0.6810,  ..., -0.0278, -0.5934, -2.6497],\n",
      "        ...,\n",
      "        [ 0.6929, -1.1271, -1.5086,  ...,  0.0370,  1.1239, -1.1810],\n",
      "        [ 0.0143,  2.1574, -0.2989,  ..., -0.2977,  1.8317,  0.3929],\n",
      "        [-0.2165, -1.9488,  0.0999,  ...,  0.1306, -3.2307, -1.2602]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa = SimpleAttention(GPT2_CONFIG)\n",
    "outputs = sa(input_vec)\n",
    "print(input_vec.shape)\n",
    "print(input_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062a609",
   "metadata": {},
   "source": [
    "**Multihead attention**\n",
    "Above one is good, for multihead mechanism, jsut put it into a list, but computation is not efficient, for efficiency we can parallize it.  \n",
    "    *Instruction: Same as above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12094452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        d_in = cfg['n_embd']\n",
    "        d_out = cfg['n_embd']\n",
    "        self.n_head = cfg['n_head']\n",
    "        \n",
    "        assert d_out % self.n_head == 0\n",
    "\n",
    "        self.head_dim = d_out // self.n_head\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_queries = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        b, n_tokens, dim = x.shape\n",
    "        # Step 1: Calculate keys, queries, values for batch\n",
    "        keys = self.W_keys(x)\n",
    "        queries = self.W_queries(x)\n",
    "        values = self.W_values(x)\n",
    "\n",
    "        # Step 2: calculate attn_scores, feed it to softmax, mask it, get weight_scores\n",
    "        keys = keys.view(b, n_tokens, self.n_head, self.head_dim)\n",
    "        queries = queries.view(b, n_tokens, self.n_head, self.head_dim)\n",
    "        values = values.view(b, n_tokens, self.n_head, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-1, -2)\n",
    "        subsequent_mask = torch.tril(torch.ones(n_tokens, n_tokens), diagonal=0)\n",
    "        attn_scores.masked_fill_(subsequent_mask == 0, -torch.inf) # mask subsequent tokens\n",
    "        if attn_mask is not None: # mask padded tokens\n",
    "            attn_scores.masked_fill_(~attn_mask, -torch.inf)\n",
    "        weight_scores = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        weighted_sum = weight_scores @ values\n",
    "\n",
    "        weighted_sum = weighted_sum.reshape(b, n_tokens, -1)\n",
    "        return weighted_sum, weight_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "459c41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blocks import InputPreprocess\n",
    "mha = MultiHeadAttention(GPT2_CONFIG)\n",
    "text = [\"Who am i?\",\n",
    "        \"Tell me what's the transformers in 5 minutes\"]\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "ip = InputPreprocess(tokenizer ,GPT2_CONFIG)\n",
    "inputs, attn_mask = ip(text)\n",
    "out_attn, weight_scores = mha(inputs, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2346827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 768])\n",
      "tensor([[[ 0.5148, -0.7446,  0.6746,  ...,  1.0137, -0.2017, -1.2037],\n",
      "         [-1.1692,  0.3488,  1.2602,  ..., -0.1721,  0.1469, -0.3785],\n",
      "         [-0.1008, -0.7582, -0.0482,  ..., -0.1435, -0.3514,  0.1864],\n",
      "         ...,\n",
      "         [ 0.2218, -0.5004,  0.1220,  ..., -0.1558,  0.7435, -0.6291],\n",
      "         [ 0.1820,  0.1435,  0.1762,  ..., -0.2816, -0.6893,  0.0477],\n",
      "         [ 0.6674,  0.8395, -0.0776,  ..., -0.1935, -0.3169,  1.3297]],\n",
      "\n",
      "        [[-0.3730, -0.9076,  1.2012,  ..., -0.2333,  0.8082, -0.0533],\n",
      "         [ 0.4610,  0.7832,  1.8192,  ..., -0.9667,  0.5596, -0.5501],\n",
      "         [-0.5123,  1.0797,  0.1452,  ...,  1.9712,  0.1371, -0.3466],\n",
      "         ...,\n",
      "         [-0.0801, -0.0315, -0.5084,  ...,  0.5341, -0.0106,  0.4413],\n",
      "         [ 0.6552, -0.8133, -0.6314,  ..., -0.0660, -0.5160, -0.1251],\n",
      "         [-0.7225,  0.2390,  0.7831,  ..., -0.4127,  0.4306, -0.2961]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out_attn.shape)\n",
    "print(out_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "780b8c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True,  True,  True,  True, False, False, False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4811190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [5.9474e-01, 4.0526e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.6968e-02, 3.7213e-04, 9.8266e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [2.3043e-06, 2.5264e-03, 9.9747e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [5.2163e-01, 4.4168e-01, 3.6689e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.5386e-01, 1.6825e-03, 4.4439e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.0000e+00, 5.5031e-07, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.6840e-08, 9.9999e-01, 1.2233e-05,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.2996e-01, 8.5078e-01, 1.9189e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [6.7106e-06, 8.9443e-01, 9.5366e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.1709e-07, 9.3679e-01, 6.3174e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.7994e-09, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.8987e-08, 9.9997e-01, 3.3127e-05,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [9.2641e-01, 1.7655e-04, 6.6267e-05,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.7121e-02, 6.4452e-08, 1.5185e-04,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [6.7592e-01, 1.2468e-06, 1.4688e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.9479e-01, 5.2060e-03, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.1722e-05, 9.9251e-01, 7.4832e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [5.9792e-01, 9.1470e-05, 3.9423e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.4869e-02, 5.0633e-08, 6.1020e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.6346e-03, 1.3382e-10, 8.0944e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [8.4468e-01, 1.5532e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.6083e-01, 1.6352e-03, 6.3754e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.0774e-05, 3.9711e-03, 1.2132e-04,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [6.9689e-01, 5.9236e-04, 5.4380e-06,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.6658e-01, 7.2096e-01, 2.3196e-05,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [5.8473e-02, 9.4153e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.3165e-04, 9.9976e-01, 1.2992e-05,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [3.6945e-06, 9.9391e-01, 6.0775e-03,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.7141e-07, 1.0000e+00, 5.1168e-08,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.1882e-05, 9.9998e-01, 7.3722e-06,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.2523e-01, 7.4772e-02, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.9673e-01, 2.3235e-03, 9.5051e-04,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [1.1639e-07, 1.6060e-01, 1.2323e-01,  ..., 3.1995e-05,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.9891e-01, 1.6347e-05, 2.7450e-04,  ..., 2.4866e-05,\n",
      "           1.6926e-11, 0.0000e+00],\n",
      "          [2.4755e-06, 6.3075e-07, 1.4823e-07,  ..., 3.0643e-03,\n",
      "           8.7649e-09, 9.9681e-01]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [5.0947e-04, 9.9949e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.1002e-03, 3.6309e-01, 6.3581e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [5.3483e-02, 4.1945e-05, 3.2871e-11,  ..., 9.3739e-01,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [8.5268e-07, 4.0847e-02, 3.9140e-05,  ..., 1.3263e-03,\n",
      "           5.6859e-04, 0.0000e+00],\n",
      "          [4.8299e-05, 1.8561e-02, 6.7873e-01,  ..., 2.9622e-05,\n",
      "           1.0268e-02, 6.6155e-06]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [5.5899e-04, 9.9944e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.4791e-02, 9.8508e-01, 1.2621e-04,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [2.0186e-02, 9.5782e-01, 2.2332e-05,  ..., 9.4945e-05,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.4425e-05, 1.0918e-05, 1.6090e-07,  ..., 2.0786e-04,\n",
      "           7.0193e-05, 0.0000e+00],\n",
      "          [3.8026e-07, 2.6420e-07, 7.6627e-06,  ..., 7.5541e-01,\n",
      "           2.2983e-01, 5.1208e-07]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [5.0632e-03, 9.9494e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.3818e-01, 3.9496e-04, 6.1425e-02,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [2.6632e-06, 1.8061e-07, 1.0000e+00,  ..., 2.0322e-08,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.3100e-05, 4.7154e-07, 9.6577e-05,  ..., 4.9950e-08,\n",
      "           9.9826e-01, 0.0000e+00],\n",
      "          [4.8164e-03, 3.2152e-04, 8.5784e-08,  ..., 2.3512e-03,\n",
      "           2.5493e-07, 1.9852e-04]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [1.7466e-01, 8.2534e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [9.3723e-05, 2.6372e-01, 7.3618e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [4.5332e-03, 6.7551e-02, 3.8279e-02,  ..., 3.9644e-01,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [2.1909e-01, 2.9437e-02, 2.4468e-04,  ..., 2.5125e-07,\n",
      "           1.7861e-04, 0.0000e+00],\n",
      "          [8.1187e-09, 1.1821e-04, 4.0860e-10,  ..., 1.1579e-06,\n",
      "           2.5936e-07, 2.8903e-02]],\n",
      "\n",
      "         [[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [8.4958e-01, 1.5042e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [7.8776e-05, 9.9992e-01, 6.0732e-07,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [9.4663e-03, 7.8418e-03, 9.7793e-01,  ..., 3.3676e-03,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [5.9518e-01, 2.5984e-01, 1.0157e-01,  ..., 2.1734e-03,\n",
      "           7.2306e-06, 0.0000e+00],\n",
      "          [2.6897e-01, 7.6802e-08, 6.1318e-04,  ..., 7.0856e-07,\n",
      "           3.9867e-07, 7.3042e-01]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(weight_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7386d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

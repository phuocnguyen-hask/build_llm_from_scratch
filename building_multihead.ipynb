{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0325462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b4c71",
   "metadata": {},
   "source": [
    "**Attention mechanism**\n",
    "1. Produce queries ,keys, values,\n",
    "2. Calculate `attention_scores`, mask those above diagonal to prevent cheating\n",
    "3. Put `attention_scores` into softmax function, then do weighted sum of values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cded4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, cfg, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        d_in = cfg['n_embd']\n",
    "        d_out = cfg['n_embd']\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Step1: produce queries, keys, values\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Step2: calculate attention_scores\n",
    "        attn_scores = queries @ keys.transpose(-1, -2)\n",
    "        mask = torch.tril(torch.ones_like(attn_scores), diagonal=0)\n",
    "        attn_scores.masked_fill_(mask==0, -torch.inf)\n",
    "\n",
    "        # Step3\n",
    "        weight_scores = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_out = weight_scores @ values\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013ddb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_CONFIG = {\n",
    "  \"activation_function\": \"gelu_new\",\n",
    "  \"architectures\": [\n",
    "    \"GPT2LMHeadModel\"\n",
    "  ],\n",
    "  \"attn_pdrop\": 0.1,\n",
    "  \"bos_token_id\": 50256,\n",
    "  \"embd_pdrop\": 0.1,\n",
    "  \"eos_token_id\": 50256,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_epsilon\": 1e-05,\n",
    "  \"model_type\": \"gpt2\",\n",
    "  \"n_ctx\": 1024,\n",
    "  \"n_embd\": 768,\n",
    "  \"n_head\": 12,\n",
    "  \"n_layer\": 12,\n",
    "  \"n_positions\": 1024,\n",
    "  \"resid_pdrop\": 0.1,\n",
    "  \"summary_activation\": None,\n",
    "  \"summary_first_dropout\": 0.1,\n",
    "  \"summary_proj_to_labels\": True,\n",
    "  \"summary_type\": \"cls_index\",\n",
    "  \"summary_use_proj\": True,\n",
    "  \"task_specific_params\": {\n",
    "    \"text-generation\": {\n",
    "      \"do_sample\": True,\n",
    "      \"max_length\": 50\n",
    "    }\n",
    "  },\n",
    "  \"vocab_size\": 50257\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad024c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "print(tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34cab7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 768])\n",
      "tensor([[-1.5461,  1.7754, -1.7155,  ..., -1.6903,  0.1360,  1.1227],\n",
      "        [-0.5807, -0.0902,  1.2370,  ..., -3.0944, -1.6663,  0.7922],\n",
      "        [ 0.1952, -0.4214, -0.4008,  ...,  0.8180, -2.9210, -0.1329],\n",
      "        ...,\n",
      "        [-0.6921, -0.5745, -0.9969,  ...,  0.9060,  2.9993,  1.8954],\n",
      "        [-1.0469, -0.6413, -0.7287,  ..., -1.4326, -0.2984,  2.2459],\n",
      "        [-1.0589,  0.3241,  2.0155,  ...,  1.3143,  0.3874,  1.2294]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(GPT2_CONFIG['vocab_size'], GPT2_CONFIG['n_embd'])\n",
    "pos_embedding_layer = nn.Embedding(GPT2_CONFIG['n_positions'], GPT2_CONFIG['n_embd'])\n",
    "txt = 'Do you know who am i?'\n",
    "tokens =  tokenizer.encode(txt)\n",
    "token_len = len(tokens)\n",
    "tokens = torch.tensor(tokens) # convert to pytorch tensor for compapility\n",
    "embeded_vec = embedding_layer(tokens)\n",
    "pos_vec = embedding_layer(torch.arange(token_len))\n",
    "input_vec = embeded_vec + pos_vec\n",
    "print(input_vec.shape)\n",
    "print(input_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b3f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 768])\n",
      "tensor([[-1.5461,  1.7754, -1.7155,  ..., -1.6903,  0.1360,  1.1227],\n",
      "        [-0.5807, -0.0902,  1.2370,  ..., -3.0944, -1.6663,  0.7922],\n",
      "        [ 0.1952, -0.4214, -0.4008,  ...,  0.8180, -2.9210, -0.1329],\n",
      "        ...,\n",
      "        [-0.6921, -0.5745, -0.9969,  ...,  0.9060,  2.9993,  1.8954],\n",
      "        [-1.0469, -0.6413, -0.7287,  ..., -1.4326, -0.2984,  2.2459],\n",
      "        [-1.0589,  0.3241,  2.0155,  ...,  1.3143,  0.3874,  1.2294]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa = SimpleAttention(GPT2_CONFIG)\n",
    "outputs = sa(input_vec)\n",
    "print(input_vec.shape)\n",
    "print(input_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062a609",
   "metadata": {},
   "source": [
    "**Multihead attention**\n",
    "Above one is good, for multihead mechanism, jsut put it into a list, but computation is not efficient, for efficiency we can parallize it.  \n",
    "    *Instruction: Same as above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12094452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        d_in = cfg['n_embd']\n",
    "        d_out = cfg['n_embd']\n",
    "        self.n_head = cfg['n_head']\n",
    "        \n",
    "        assert d_out % self.n_head == 0\n",
    "\n",
    "        self.head_dim = d_out // self.n_head\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_queries = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_values = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, n_tokens, dim = x.shape\n",
    "        # Step 1: Calculate keys, queries, values for batch\n",
    "        keys = self.W_keys(x)\n",
    "        queries = self.W_queries(x)\n",
    "        values = self.W_values(x)\n",
    "\n",
    "        # Step 2: calculate attn_scores, feed it to softmax, mask it, get weight_scores\n",
    "        keys = keys.view(b, n_tokens, self.n_head, self.head_dim)\n",
    "        queries = queries.view(b, n_tokens, self.n_head, self.head_dim)\n",
    "        values = values.view(b, n_tokens, self.n_head, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = keys @ queries.transpose(-1, -2)\n",
    "        weight_scores = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        weighted_sum = weight_scores @ keys\n",
    "\n",
    "        weighted_sum = weighted_sum.reshape(b, n_tokens, -1)\n",
    "        return weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "459c41f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(GPT2_CONFIG)\n",
    "text = \"Who am i?\"\n",
    "ids = tokenizer.encode(text)\n",
    "context_length = len(ids)\n",
    "ids = torch.tensor(ids) # convert to tensor pytorch for compability\n",
    "embed_vec = embedding_layer(ids)\n",
    "pos_vec = pos_embedding_layer(torch.arange(context_length))\n",
    "input_vec = embed_vec + pos_vec\n",
    "print(input_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df451442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "input_vec = input_vec.unsqueeze(0)\n",
    "mha_output = mha(input_vec)\n",
    "print(mha_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2346827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
